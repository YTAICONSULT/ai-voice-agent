<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simple AI Voice Agent</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; display: flex; flex-direction: column; align-items: center; justify-content: space-between; min-height: 80vh; background-color: #f0f2f5; color: #333; }
        h1 { color: #2c3e50; margin-bottom: 30px; }
        #controls { margin-top: auto; margin-bottom: 20px; }
        #controls button { margin: 10px; padding: 15px 30px; font-size: 18px; cursor: pointer; border: none; border-radius: 5px; transition: background-color 0.3s ease; }
        #startButton { background-color: #28a745; color: white; }
        #startButton:hover:not(:disabled) { background-color: #218838; }
        #stopListeningButton { background-color: #dc3545; color: white; }
        #stopListeningButton:hover:not(:disabled) { background-color: #c82333; }
        #controls button:disabled { background-color: #cccccc; cursor: not-allowed; color: #666; }

        #status { display: none; } /* Hide the status text */

        #visualIndicator { 
            width: 120px; 
            height: 120px; 
            border-radius: 50%; 
            margin-top: 40px; 
            transition: background-color 0.5s ease, transform 0.5s ease; 
            display: flex; 
            align-items: center; 
            justify-content: center;
            font-size: 18px; 
            color: white;
            text-transform: uppercase;
            font-weight: bold;
            flex-shrink: 0; 
        }

        /* Animations for different states */
        .status-ready { background-color: #6c757d; }
        .status-listening { background-color: #007bff; animation: pulse-blue 1.5s infinite alternate; } /* Blue */
        .status-user-talking { background-color: #17a2b8; animation: pulse-purple 0.5s infinite alternate; } /* A different shade of blue for user talking */
        .status-ai-thinking { background-color: #ffc107; animation: spin 2s linear infinite; } /* Orange */
        .status-ai-answering { background-color: #007bff; animation: pulse-green 0.8s infinite alternate; } /* Blue */
        .status-error { background-color: #dc3545; }

        @keyframes pulse-blue {
            from { transform: scale(0.9); opacity: 0.7; }
            to { transform: scale(1.1); opacity: 1; }
        }
        @keyframes pulse-purple {
            from { transform: scale(0.95); opacity: 0.8; }
            to { transform: scale(1.05); opacity: 1; }
        }
        @keyframes spin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <h1>AI Voice Agent</h1>

    <div id="visualIndicator"></div>

    <div id="controls">
        <button id="startButton">Start Conversation</button>
        <button id="stopListeningButton" disabled>Stop Conversation</button>
    </div>

    <audio id="audioPlayback"></audio> <!-- Hidden audio player -->

    <script>
        document.addEventListener('DOMContentLoaded', async (event) => {
            const startButton = document.getElementById('startButton');
            const stopListeningButton = document.getElementById('stopListeningButton'); 
            const audioPlayback = document.getElementById('audioPlayback');
            const statusDiv = document.getElementById('status'); 
            const visualIndicator = document.getElementById('visualIndicator');

            // Load audio configuration from server
            try {
                const configResponse = await fetch('/config');
                if (configResponse.ok) {
                    const config = await configResponse.json();
                    SILENCE_THRESHOLD = config.SILENCE_THRESHOLD;
                    BARGE_IN_THRESHOLD = config.BARGE_IN_THRESHOLD;
                    SILENCE_DURATION = config.SILENCE_DURATION;
                    SAMPLE_RATE = config.SAMPLE_RATE;
                    console.log('Audio configuration loaded:', config);
                } else {
                    console.warn('Failed to load audio configuration, using defaults');
                }
            } catch (error) {
                console.warn('Error loading audio configuration:', error, 'Using defaults');
            }

            let audioContext;
            let mediaStreamSource;
            let analyser;
            let scriptProcessor;
            let mediaRecorder;
            let audioChunks = [];
            let isRecording = false;
            let silenceStart = 0;
            let isModelSpeaking = false; 
            let isProcessingAudio = false; 
            let currentStream; 
            let sessionId = crypto.randomUUID(); 
            let abortController; 
            let isConversationStopped = true; 

            // Audio configuration will be loaded from server
            let SILENCE_THRESHOLD = -40; 
            let BARGE_IN_THRESHOLD = -30; 
            let SILENCE_DURATION = 1000; 
            let SAMPLE_RATE = 44100; 

            function updateStatus(text, indicatorClass = 'status-ready') {
                visualIndicator.className = indicatorClass;
                // visualIndicator.textContent = text.split(' ')[0]; // Removed this line
            }

            function stopListening() {
                console.log("stopListening function called.");
                isConversationStopped = true; 

                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                    console.log("MediaRecorder stopped.");
                }
                if (currentStream) {
                    currentStream.getTracks().forEach(track => track.stop());
                    console.log("Microphone stream tracks stopped.");
                }
                if (scriptProcessor) {
                    scriptProcessor.disconnect();
                    console.log("ScriptProcessor disconnected.");
                }
                if (analyser) {
                    analyser.disconnect();
                }
                if (mediaStreamSource) {
                    mediaStreamSource.disconnect();
                    console.log("MediaStreamSource disconnected.");
                }
                if (audioContext) {
                    audioContext.close();
                }

                if (abortController) {
                    abortController.abort();
                    console.log("Ongoing fetch request aborted.");
                }

                audioPlayback.pause();
                audioPlayback.currentTime = 0; 
                audioPlayback.src = '';
                audioPlayback.load(); 
                audioPlayback.removeEventListener('ended', audioPlaybackEndedHandler); 

                isRecording = false;
                isModelSpeaking = false;
                isProcessingAudio = false;
                audioChunks = [];

                startButton.disabled = false;
                stopListeningButton.disabled = true;
                updateStatus("Ready", 'status-ready');
                console.log("Status set to 'Ready'.");
            }

            const audioPlaybackEndedHandler = () => {
                console.log("audioPlaybackEndedHandler triggered. isConversationStopped:", isConversationStopped);
                if (!stopListeningButton.disabled) { 
                    isModelSpeaking = false; 
                    updateStatus("Listening...", 'status-listening');
                    console.log("Status updated to Listening... from audioPlaybackEndedHandler.");
                } else {
                    console.log("Audio playback ended, but conversation is manually stopped. Status not changed by this handler.");
                }
            };

            async function startListening() {
                console.log("startListening function called.");
                if (isRecording) {
                    console.log("Already recording, returning.");
                    return; 
                }
                isConversationStopped = false; 

                try {
                    console.log("Requesting microphone access...");
                    currentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log("Microphone access granted.");
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    mediaStreamSource = audioContext.createMediaStreamSource(currentStream);
                    analyser = audioContext.createAnalyser();
                    scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1); 

                    mediaStreamSource.connect(analyser);
                    analyser.connect(scriptProcessor);
                    scriptProcessor.connect(audioContext.destination);

                    mediaRecorder = new MediaRecorder(currentStream);
                    audioChunks = [];

                    mediaRecorder.ondataavailable = event => {
                        audioChunks.push(event.data);
                    };

                    mediaRecorder.onstop = async () => {
                        console.log("mediaRecorder onstop triggered.");
                        isProcessingAudio = true; 

                        if (audioChunks.length === 0) {
                            console.log("No audio chunks, returning to listening.");
                            updateStatus("Listening...", 'status-listening');
                            isProcessingAudio = false; 
                            return; 
                        }

                        const audioBlob = new Blob(audioChunks, { type: 'audio/m4m' }); 
                        const formData = new FormData();
                        formData.append('audio', audioBlob, 'recording.m4m');
                        formData.append('session_id', sessionId); 

                        updateStatus("Thinking...", 'status-ai-thinking');
                        console.log("Sending audio to server for processing.");

                        abortController = new AbortController();
                        const signal = abortController.signal;

                        try {
                            const response = await fetch('/process_audio', {
                                method: 'POST',
                                body: formData,
                                signal: signal 
                            });

                            if (response.ok) {
                                const responseData = await response.json(); 
                                console.log("Received response data from server.");

                                if (!isConversationStopped) {
                                    const audioBlobResponse = new Blob([new Uint8Array(responseData.audio_data)], { type: 'audio/wav' }); 
                                    const audioUrl = URL.createObjectURL(audioBlobResponse);
                                    audioPlayback.src = audioUrl;
                                    isModelSpeaking = true; 
                                    audioPlayback.play();
                                    updateStatus("Answering...", 'status-ai-answering');
                                    console.log("AI Answering... isModelSpeaking set to true, audio playing.");
                                } else {
                                    console.log("Response received, but conversation was stopped. Not playing audio.");
                                }
                            } else {
                                if (response.statusText === 'abort') {
                                    console.log("Fetch request was aborted.");
                                } else {
                                    const errorData = await response.json();
                                    updateStatus(`Error`, 'status-error');
                                    console.error("Server error:", errorData);
                                }
                            }
                        } catch (error) {
                            if (error.name === 'AbortError') {
                                console.log("Fetch aborted by user action.");
                            }
                            else if (error.message.includes("Failed to fetch")) {
                                updateStatus(`Network Error`, 'status-error');
                                console.error("Network or fetch error:", error);
                            } else {
                                updateStatus(`Error`, 'status-error');
                                console.error("General error:", error);
                            }
                        }
                        finally {
                            audioChunks = [];
                            isRecording = false;
                            isProcessingAudio = false; 
                            console.log("Processing finished, state reset.");
                        }
                    };

                    audioPlayback.addEventListener('ended', audioPlaybackEndedHandler); 

                    scriptProcessor.onaudioprocess = function(event) {
                        if (isProcessingAudio) return;

                        const inputBuffer = event.inputBuffer.getChannelData(0);
                        let sum = 0.0;
                        for (let i = 0; i < inputBuffer.length; i++) {
                            sum += inputBuffer[i] * inputBuffer[i];
                        }
                        const rms = Math.sqrt(sum / inputBuffer.length);
                        const db = 20 * Math.log10(rms); 

                        if (isModelSpeaking) {
                            if (db > BARGE_IN_THRESHOLD) { 
                                audioPlayback.pause();
                                audioPlayback.currentTime = 0; 
                                audioPlayback.src = ''; 
                                audioPlayback.load(); 
                                isModelSpeaking = false; 
                                updateStatus("Talking...", 'status-user-talking');
                                console.log("Barge-in detected, AI audio cut.");
                            }
                            return; 
                        }

                        if (db > SILENCE_THRESHOLD) {
                            if (!isRecording) {
                                mediaRecorder.start();
                                isRecording = true;
                                updateStatus("Talking...", 'status-user-talking');
                                console.log("Recording started.");
                            }
                            silenceStart = audioContext.currentTime; 
                        } else {
                            if (isRecording && (audioContext.currentTime - silenceStart) * 1000 > SILENCE_DURATION) {
                                mediaRecorder.stop();
                                isRecording = false; 
                                updateStatus("Thinking...", 'status-ai-thinking');
                                console.log("Silence detected, recording stopped.");
                            }
                        }
                    };

                    startButton.disabled = true;
                    stopListeningButton.disabled = false; 
                    updateStatus("Listening...", 'status-listening');
                    console.log("Microphone initialized, status Listening.");

                } catch (err) {
                    updateStatus(`Error`, 'status-error');
                    console.error("Error during startListening:", err);
                }
            }

            startButton.addEventListener('click', startListening);
            stopListeningButton.addEventListener('click', stopListening); 

            updateStatus("Ready", 'status-ready');
        });
    </script>
</body>
</html>